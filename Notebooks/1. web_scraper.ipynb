response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Initialize empty DataFrame to store all data
df_full = pd.DataFrame()

for link in soup.find_all('a', href=re.compile(r'\.csv$', re.IGNORECASE)):
    csv_url = base_url + link.get('href')
    print(f"Processing: {csv_url}")
    
    try:
        # Read the CSV
        temp_df = pd.read_csv(csv_url)
        
        # If first CSV, use it as base
        if df_full.empty:
            df_full = temp_df
        else:
            # Concatenate with existing data
            df_full = pd.concat([df_full, temp_df], ignore_index=True)
            
    except Exception as e:
        print(f"Error processing {csv_url}: {e}")
        continue

# After loop completes
print(f"Final DataFrame shape: {df_full.shape}")
print(df_full.head())
